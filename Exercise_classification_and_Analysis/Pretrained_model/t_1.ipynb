{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6800cf3b-f17e-4ac7-91bb-d2bfa39346d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.48.0)\n",
      "Requirement already satisfied: decord in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: torch in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.7.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers decord numpy torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "534cceec-9e9b-4302-8742-99cca10ba856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoMAEForVideoClassification(\n",
       "  (videomae): VideoMAEModel(\n",
       "    (embeddings): VideoMAEEmbeddings(\n",
       "      (patch_embeddings): VideoMAEPatchEmbeddings(\n",
       "        (projection): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
       "      )\n",
       "    )\n",
       "    (encoder): VideoMAEEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x VideoMAELayer(\n",
       "          (attention): VideoMAESdpaAttention(\n",
       "            (attention): VideoMAESdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=22, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import VideoMAEForVideoClassification, VideoMAEImageProcessor\n",
    "from decord import VideoReader\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load model and feature extractor\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\n",
    "    r\"F:\\quest_digiflex\\exp\\tf\\videomae-finetuned\",  # or wherever your config.json and safetensors are stored\n",
    "    local_files_only=True\n",
    ")\n",
    "feature_extractor = VideoMAEImageProcessor.from_pretrained(\n",
    "    \"MCG-NJU/videomae-base\"\n",
    ")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "781b7478-a8e3-400d-b967-06b96a558fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_video_class(video_path, model, extractor, num_frames=16, id2label=None):\n",
    "    vr = VideoReader(video_path)\n",
    "    total_frames = len(vr)\n",
    "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "    frames = vr.get_batch(frame_indices).asnumpy()  # shape: (num_frames, H, W, 3)\n",
    "\n",
    "    # Extract features\n",
    "    inputs = extractor(list(frames), return_tensors=\"pt\")\n",
    "    pixel_values = inputs[\"pixel_values\"].to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "        logits = outputs.logits\n",
    "        predicted_class = logits.argmax(-1).item()\n",
    "\n",
    "    # Decode class\n",
    "    if id2label:\n",
    "        return id2label[predicted_class]\n",
    "    return predicted_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87a452bd-075c-46e6-a599-6c4c4fe9c9d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "':' expected after dictionary key (2496193448.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    id2label = {0: \"jumping_jack\", 1: \"squat\", 2: \"pushup\", ...}  # example\u001b[0m\n\u001b[1;37m                                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m ':' expected after dictionary key\n"
     ]
    }
   ],
   "source": [
    "video_path = r\"F:\\quest_digiflex\\exp\\tf\\videomae-finetuned\\Bodyweight Squats.mp4\"\n",
    "\n",
    "# You must define this mapping (same used during training)\n",
    "id2label = {0: \"jumping_jack\", 1: \"squat\", 2: \"pushup\", ...}  # example\n",
    "\n",
    "predicted_class = predict_video_class(video_path, model, feature_extractor, id2label=id2label)\n",
    "print(\"Predicted class:\", predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94c30d12-2f6f-414f-aef6-7d88b4e763e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Predicted Exercise Class: squat\n"
     ]
    }
   ],
   "source": [
    "from transformers import VideoMAEForVideoClassification, VideoMAEImageProcessor, AutoConfig\n",
    "from decord import VideoReader\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# --- Load Model, Config, Feature Extractor ---\n",
    "model_dir = r\"F:/quest_digiflex/exp/tf/videomae-finetuned\"\n",
    "\n",
    "# Make sure model_10.safetensor is renamed to model.safetensors before running this\n",
    "model = VideoMAEForVideoClassification.from_pretrained(model_dir, local_files_only=True)\n",
    "feature_extractor = VideoMAEImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "config = AutoConfig.from_pretrained(model_dir, local_files_only=True)\n",
    "id2label = config.id2label  # Automatically retrieved from saved config\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# --- Inference Function ---\n",
    "def predict_video_class(video_path, model, extractor, num_frames=16, id2label=None):\n",
    "    vr = VideoReader(video_path)\n",
    "    total_frames = len(vr)\n",
    "    \n",
    "    if total_frames < num_frames:\n",
    "        raise ValueError(f\"Video has only {total_frames} frames, but {num_frames} are required.\")\n",
    "\n",
    "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "    frames = vr.get_batch(frame_indices).asnumpy()  # (num_frames, H, W, 3)\n",
    "\n",
    "    inputs = extractor(list(frames), return_tensors=\"pt\")\n",
    "    pixel_values = inputs[\"pixel_values\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "        logits = outputs.logits\n",
    "        predicted_class = logits.argmax(-1).item()\n",
    "\n",
    "    return id2label[predicted_class] if id2label else predicted_class\n",
    "\n",
    "# --- Predict on Unseen Video ---\n",
    "video_path = r\"F:\\quest_digiflex\\exp\\tf\\videomae-finetuned\\Bodyweight Squats.mp4\"  # Update this to your actual video path\n",
    "\n",
    "predicted_label = predict_video_class(video_path, model, feature_extractor, id2label=id2label)\n",
    "print(\"✅ Predicted Exercise Class:\", predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93b337d3-c0a1-4f64-9af0-192bd699cdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Predicted Exercise Class: tricep dips\n"
     ]
    }
   ],
   "source": [
    "# --- Predict on Unseen Video ---\n",
    "video_path = r\"F:\\quest_digiflex\\exp\\tf\\videomae-finetuned\\test\\pull Up\\pull up_1.mp4\"  # Update this to your actual video path\n",
    "\n",
    "predicted_label = predict_video_class(video_path, model, feature_extractor, id2label=id2label)\n",
    "print(\"✅ Predicted Exercise Class:\", predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d49cc53f-59eb-493e-b2fa-8ab951d9fbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Predicted Exercise Class: pull Up\n"
     ]
    }
   ],
   "source": [
    "# --- Predict on Unseen Video ---\n",
    "video_path = r\"F:\\quest_digiflex\\exp\\tf\\videomae-finetuned\\test\\pull Up\\pull up_2.mp4\"  # Update this to your actual video path\n",
    "\n",
    "predicted_label = predict_video_class(video_path, model, feature_extractor, id2label=id2label)\n",
    "print(\"✅ Predicted Exercise Class:\", predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1eb787e-ef18-4f89-85a6-19fdfbebea44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Predicted Exercise Class: russian twist\n"
     ]
    }
   ],
   "source": [
    "# --- Predict on Unseen Video ---\n",
    "video_path = r\"F:\\quest_digiflex\\exp\\tf\\videomae-finetuned\\test\\push-up\\push-up_1.mp4\"  # Update this to your actual video path\n",
    "\n",
    "predicted_label = predict_video_class(video_path, model, feature_extractor, id2label=id2label)\n",
    "print(\"✅ Predicted Exercise Class:\", predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31097a91-4d93-405d-9f87-2c11d154b1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Predicted Exercise Class: push-up\n"
     ]
    }
   ],
   "source": [
    "# --- Predict on Unseen Video ---\n",
    "video_path = r\"F:\\quest_digiflex\\exp\\tf\\videomae-finetuned\\test\\push-up\\video15.mp4\"  # Update this to your actual video path\n",
    "\n",
    "predicted_label = predict_video_class(video_path, model, feature_extractor, id2label=id2label)\n",
    "print(\"✅ Predicted Exercise Class:\", predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a29eb6d-ae15-4fa5-a5ce-b1572fd45ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ barbell biceps curl_1.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ video1.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ bench press_1.mp4 | Actual: bench press --> Predicted: lat pulldown\n",
      "✅ bench press_2.mp4 | Actual: bench press --> Predicted: bench press\n",
      "✅ video2.mp4 | Actual: bench press --> Predicted: bench press\n",
      "✅ chest fly machine_1.mp4 | Actual: chest fly machine --> Predicted: chest fly machine\n",
      "✅ chest fly machine_2.mp4 | Actual: chest fly machine --> Predicted: barbell biceps curl\n",
      "✅ chest fly machine_3.mp4 | Actual: chest fly machine --> Predicted: chest fly machine\n",
      "✅ video3.mp4 | Actual: chest fly machine --> Predicted: chest fly machine\n",
      "✅ deadlift_1.mp4 | Actual: deadlift --> Predicted: russian twist\n",
      "✅ video4.mp4 | Actual: deadlift --> Predicted: tricep dips\n",
      "✅ declince bench press_1.mp4 | Actual: decline bench press --> Predicted: bench press\n",
      "✅ decline bench press_2.mp4 | Actual: decline bench press --> Predicted: bench press\n",
      "✅ video5.mp4 | Actual: decline bench press --> Predicted: plank\n",
      "✅ hammer curl_1.mp4 | Actual: hammer curl --> Predicted: lateral raise\n",
      "✅ hammer curl_2.mp4 | Actual: hammer curl --> Predicted: barbell biceps curl\n",
      "✅ video6.mp4 | Actual: hammer curl --> Predicted: leg extension\n",
      "✅ hip thrush_1.mp4 | Actual: hip thrust --> Predicted: leg extension\n",
      "✅ hip thrust_1.mp4 | Actual: hip thrust --> Predicted: deadlift\n",
      "✅ video7.mp4 | Actual: hip thrust --> Predicted: plank\n",
      "✅ incline bench press_1.mp4 | Actual: incline bench press --> Predicted: bench press\n",
      "✅ incline bench press_2.mp4 | Actual: incline bench press --> Predicted: romanian deadlift\n",
      "✅ video8.mp4 | Actual: incline bench press --> Predicted: lat pulldown\n",
      "✅ lat pulldown_1.mp4 | Actual: lat pulldown --> Predicted: t bar row\n",
      "✅ lat pulldown_2.mp4 | Actual: lat pulldown --> Predicted: lat pulldown\n",
      "✅ video9.mp4 | Actual: lat pulldown --> Predicted: incline bench press\n",
      "✅ lateral raise_1.mp4 | Actual: lateral raise --> Predicted: lateral raise\n",
      "✅ video10.mp4 | Actual: lateral raise --> Predicted: lateral raise\n",
      "✅ leg extension_1.mp4 | Actual: leg extension --> Predicted: leg extension\n",
      "✅ leg extension_2.mp4 | Actual: leg extension --> Predicted: leg extension\n",
      "✅ video11.mp4 | Actual: leg extension --> Predicted: leg extension\n",
      "✅ leg raises_1.mp4 | Actual: leg raises --> Predicted: leg raises\n",
      "✅ leg raises_2.mp4 | Actual: leg raises --> Predicted: barbell biceps curl\n",
      "✅ video12.mp4 | Actual: leg raises --> Predicted: barbell biceps curl\n",
      "✅ plank_1.mp4 | Actual: plank --> Predicted: russian twist\n",
      "✅ video13.mp4 | Actual: plank --> Predicted: hip thrust\n",
      "✅ pull up_1.mp4 | Actual: pull Up --> Predicted: tricep dips\n",
      "✅ pull up_2.mp4 | Actual: pull Up --> Predicted: pull Up\n",
      "✅ video14.mp4 | Actual: pull Up --> Predicted: russian twist\n",
      "✅ push-up_1.mp4 | Actual: push-up --> Predicted: russian twist\n",
      "✅ video15.mp4 | Actual: push-up --> Predicted: push-up\n",
      "✅ romanian deadlift_1.mp4 | Actual: romanian deadlift --> Predicted: squat\n",
      "✅ romanian deadlift_2.mp4 | Actual: romanian deadlift --> Predicted: t bar row\n",
      "✅ video16.mp4 | Actual: romanian deadlift --> Predicted: squat\n",
      "✅ russian twist_1.mp4 | Actual: russian twist --> Predicted: hip thrust\n",
      "✅ russian twist_2.mp4 | Actual: russian twist --> Predicted: russian twist\n",
      "✅ video17t.mp4 | Actual: russian twist --> Predicted: shoulder press\n",
      "✅ shoulder press_1.mp4 | Actual: shoulder press --> Predicted: t bar row\n",
      "✅ shoulder press_2.mp4 | Actual: shoulder press --> Predicted: lat pulldown\n",
      "✅ video18.mp4 | Actual: shoulder press --> Predicted: lat pulldown\n",
      "✅ squat_1.mp4 | Actual: squat --> Predicted: bench press\n",
      "✅ squat_2.mp4 | Actual: squat --> Predicted: incline bench press\n",
      "✅ video19.mp4 | Actual: squat --> Predicted: russian twist\n",
      "✅ t bar row_2.mp4 | Actual: t bar row --> Predicted: romanian deadlift\n",
      "✅ t bat row_1.mp4 | Actual: t bar row --> Predicted: tricep dips\n",
      "✅ video20.mp4 | Actual: t bar row --> Predicted: romanian deadlift\n",
      "✅ tricep dips_1.mp4 | Actual: tricep dips --> Predicted: pull Up\n",
      "✅ tricep dips_2.mp4 | Actual: tricep dips --> Predicted: t bar row\n",
      "✅ video21.mp4 | Actual: tricep dips --> Predicted: pull Up\n",
      "✅ tricep pushdown_1.mp4 | Actual: tricep Pushdown --> Predicted: barbell biceps curl\n",
      "✅ video22.mp4 | Actual: tricep Pushdown --> Predicted: tricep Pushdown\n",
      "\n",
      "=== Prediction Summary ===\n",
      "                        video         actual_class      predicted_class  \\\n",
      "0   barbell biceps curl_1.mp4  barbell biceps curl  barbell biceps curl   \n",
      "1                  video1.mp4  barbell biceps curl  barbell biceps curl   \n",
      "2           bench press_1.mp4          bench press         lat pulldown   \n",
      "3           bench press_2.mp4          bench press          bench press   \n",
      "4                  video2.mp4          bench press          bench press   \n",
      "..                        ...                  ...                  ...   \n",
      "56          tricep dips_1.mp4          tricep dips              pull Up   \n",
      "57          tricep dips_2.mp4          tricep dips            t bar row   \n",
      "58                video21.mp4          tricep dips              pull Up   \n",
      "59      tricep pushdown_1.mp4      tricep Pushdown  barbell biceps curl   \n",
      "60                video22.mp4      tricep Pushdown      tricep Pushdown   \n",
      "\n",
      "    is_correct  \n",
      "0         True  \n",
      "1         True  \n",
      "2        False  \n",
      "3         True  \n",
      "4         True  \n",
      "..         ...  \n",
      "56       False  \n",
      "57       False  \n",
      "58       False  \n",
      "59       False  \n",
      "60        True  \n",
      "\n",
      "[61 rows x 4 columns]\n",
      "\n",
      "=== Statistics ===\n",
      "🎞️ Total videos processed     : 61\n",
      "✅ Correct predictions        : 18\n",
      "❌ Incorrect predictions      : 43\n",
      "⚠️ Most misclassified class   : squat\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# --- Test Directory ---\n",
    "test_root = r\"F:\\quest_digiflex\\exp\\tf\\videomae-finetuned\\test\"\n",
    "\n",
    "# --- Store results ---\n",
    "results = []\n",
    "\n",
    "# --- Process each video ---\n",
    "for actual_class in os.listdir(test_root):\n",
    "    class_path = os.path.join(test_root, actual_class)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    video_files = glob(os.path.join(class_path, \"*.mp4\"))\n",
    "\n",
    "    for video_path in video_files:\n",
    "        try:\n",
    "            predicted_label = predict_video_class(video_path, model, feature_extractor, id2label=id2label)\n",
    "            video_name = os.path.basename(video_path)\n",
    "            results.append({\n",
    "                \"video\": video_name,\n",
    "                \"actual_class\": actual_class,\n",
    "                \"predicted_class\": predicted_label\n",
    "            })\n",
    "            print(f\"✅ {video_name} | Actual: {actual_class} --> Predicted: {predicted_label}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing {video_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "# --- Convert to DataFrame ---\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# --- Accuracy Analysis ---\n",
    "df_results[\"is_correct\"] = df_results[\"actual_class\"] == df_results[\"predicted_class\"]\n",
    "\n",
    "total_videos = len(df_results)\n",
    "correct_preds = df_results[\"is_correct\"].sum()\n",
    "incorrect_preds = total_videos - correct_preds\n",
    "\n",
    "# --- Most misclassified class ---\n",
    "incorrect_df = df_results[df_results[\"is_correct\"] == False]\n",
    "most_misclassified_class = incorrect_df[\"actual_class\"].value_counts().idxmax() if not incorrect_df.empty else None\n",
    "\n",
    "# --- Output Summary ---\n",
    "print(\"\\n=== Prediction Summary ===\")\n",
    "print(df_results)\n",
    "\n",
    "print(\"\\n=== Statistics ===\")\n",
    "print(f\"🎞️ Total videos processed     : {total_videos}\")\n",
    "print(f\"✅ Correct predictions        : {correct_preds}\")\n",
    "print(f\"❌ Incorrect predictions      : {incorrect_preds}\")\n",
    "if most_misclassified_class:\n",
    "    print(f\"⚠️ Most misclassified class   : {most_misclassified_class}\")\n",
    "\n",
    "# --- Optional: Save to CSV ---\n",
    "# df_results.to_csv(\"video_predictions_with_accuracy.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "889adead-bf61-4d89-bfb6-91ab14d2a4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ barbell biceps curl_1.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_10.mp4 | Actual: barbell biceps curl --> Predicted: tricep Pushdown\n",
      "✅ barbell biceps curl_11.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_12.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_13.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_14.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_15.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_16.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_17.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_18.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_19.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_2.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_20.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_21.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_22.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_23.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_24.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_25.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_26.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_27.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_28.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_29.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_3.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_30.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_31.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_32.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_33.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_34.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_35.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_36.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_37.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_38.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_39.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_4.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_40.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_41.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_42.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_43.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_44.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_45.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_46.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_47.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_48.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_49.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_5.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_50.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_51.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_52.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_53.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_54.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_55.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_56.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_57.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_58.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_59.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_6.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_60.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_61.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_62.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_7.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_8.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ barbell biceps curl_9.mp4 | Actual: barbell biceps curl --> Predicted: barbell biceps curl\n",
      "✅ bench press_1.mp4 | Actual: bench press --> Predicted: bench press\n",
      "✅ bench press_10.mp4 | Actual: bench press --> Predicted: bench press\n",
      "✅ bench press_11.mp4 | Actual: bench press --> Predicted: bench press\n",
      "✅ bench press_12.mp4 | Actual: bench press --> Predicted: bench press\n",
      "✅ bench press_13.mp4 | Actual: bench press --> Predicted: bench press\n",
      "✅ bench press_14.mp4 | Actual: bench press --> Predicted: incline bench press\n",
      "✅ bench press_15.mp4 | Actual: bench press --> Predicted: bench press\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function VideoReader.__del__ at 0x00000190096C2D40>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\prana\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\decord\\video_reader.py\", line 67, in __del__\n",
      "    _CAPI_VideoReaderFree(self._handle)\n",
      "  File \"C:\\Users\\prana\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\decord\\_ffi\\_ctypes\\function.py\", line 173, in __call__\n",
      "    check_call(_LIB.DECORDFuncCall(\n",
      "               ^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ bench press_16.mp4 | Actual: bench press --> Predicted: bench press\n",
      "✅ bench press_17.mp4 | Actual: bench press --> Predicted: bench press\n",
      "✅ bench press_18.mp4 | Actual: bench press --> Predicted: bench press\n",
      "✅ bench press_19.mp4 | Actual: bench press --> Predicted: bench press\n",
      "✅ bench press_2.mp4 | Actual: bench press --> Predicted: bench press\n",
      "✅ bench press_20.mp4 | Actual: bench press --> Predicted: bench press\n",
      "✅ bench press_21.mp4 | Actual: bench press --> Predicted: bench press\n",
      "✅ bench press_22.mp4 | Actual: bench press --> Predicted: bench press\n",
      "✅ bench press_23.mp4 | Actual: bench press --> Predicted: bench press\n",
      "✅ bench press_24.mp4 | Actual: bench press --> Predicted: bench press\n",
      "✅ bench press_25.mp4 | Actual: bench press --> Predicted: bench press\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video_path \u001b[38;5;129;01min\u001b[39;00m video_files:\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m         predicted_label \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_video_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid2label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mid2label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m         video_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(video_path)\n\u001b[0;32m     24\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     25\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m\"\u001b[39m: video_name,\n\u001b[0;32m     26\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual_class\u001b[39m\u001b[38;5;124m\"\u001b[39m: actual_class,\n\u001b[0;32m     27\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted_class\u001b[39m\u001b[38;5;124m\"\u001b[39m: predicted_label\n\u001b[0;32m     28\u001b[0m         })\n",
      "Cell \u001b[1;32mIn[8], line 36\u001b[0m, in \u001b[0;36mpredict_video_class\u001b[1;34m(video_path, model, extractor, num_frames, id2label)\u001b[0m\n\u001b[0;32m     33\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 36\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m     38\u001b[0m     predicted_class \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\videomae\\modeling_videomae.py:1086\u001b[0m, in \u001b[0;36mVideoMAEForVideoClassification.forward\u001b[1;34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;124;03m    Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;124;03meating spaghetti\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1086\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideomae\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1092\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1094\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_norm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\videomae\\modeling_videomae.py:697\u001b[0m, in \u001b[0;36mVideoMAEModel.forward\u001b[1;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    693\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    695\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values, bool_masked_pos)\n\u001b[1;32m--> 697\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    704\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\videomae\\modeling_videomae.py:481\u001b[0m, in \u001b[0;36mVideoMAEEncoder.forward\u001b[1;34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    474\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    475\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    476\u001b[0m         hidden_states,\n\u001b[0;32m    477\u001b[0m         layer_head_mask,\n\u001b[0;32m    478\u001b[0m         output_attentions,\n\u001b[0;32m    479\u001b[0m     )\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 481\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    483\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\videomae\\modeling_videomae.py:425\u001b[0m, in \u001b[0;36mVideoMAELayer.forward\u001b[1;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    421\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    422\u001b[0m     head_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    423\u001b[0m     output_attentions: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    424\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor], Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[1;32m--> 425\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayernorm_before\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# in VideoMAE, layernorm is applied before self-attention\u001b[39;49;00m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    430\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    431\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\videomae\\modeling_videomae.py:354\u001b[0m, in \u001b[0;36mVideoMAEAttention.forward\u001b[1;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    351\u001b[0m     head_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    352\u001b[0m     output_attentions: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    353\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor], Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[1;32m--> 354\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    356\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    358\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\videomae\\modeling_videomae.py:278\u001b[0m, in \u001b[0;36mVideoMAESdpaSelfAttention.forward\u001b[1;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28mself\u001b[39m, hidden_states, head_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, output_attentions: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor], Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[0;32m    277\u001b[0m     k_bias \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_bias, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 278\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m     values \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mhidden_states, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_bias)\n\u001b[0;32m    280\u001b[0m     queries \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mhidden_states, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery\u001b[38;5;241m.\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_bias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# --- Test Directory ---\n",
    "test_root = r\"F:\\quest_digiflex\\exp\\data\\archive\"\n",
    "\n",
    "# --- Store results ---\n",
    "results = []\n",
    "\n",
    "# --- Process each video ---\n",
    "for actual_class in os.listdir(test_root):\n",
    "    class_path = os.path.join(test_root, actual_class)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    video_files = glob(os.path.join(class_path, \"*.mp4\"))\n",
    "\n",
    "    for video_path in video_files:\n",
    "        try:\n",
    "            predicted_label = predict_video_class(video_path, model, feature_extractor, id2label=id2label)\n",
    "            video_name = os.path.basename(video_path)\n",
    "            results.append({\n",
    "                \"video\": video_name,\n",
    "                \"actual_class\": actual_class,\n",
    "                \"predicted_class\": predicted_label\n",
    "            })\n",
    "            print(f\"✅ {video_name} | Actual: {actual_class} --> Predicted: {predicted_label}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing {video_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "# --- Convert to DataFrame ---\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# --- Accuracy Analysis ---\n",
    "df_results[\"is_correct\"] = df_results[\"actual_class\"] == df_results[\"predicted_class\"]\n",
    "\n",
    "total_videos = len(df_results)\n",
    "correct_preds = df_results[\"is_correct\"].sum()\n",
    "incorrect_preds = total_videos - correct_preds\n",
    "\n",
    "# --- Most misclassified class ---\n",
    "incorrect_df = df_results[df_results[\"is_correct\"] == False]\n",
    "most_misclassified_class = incorrect_df[\"actual_class\"].value_counts().idxmax() if not incorrect_df.empty else None\n",
    "\n",
    "# --- Output Summary ---\n",
    "print(\"\\n=== Prediction Summary ===\")\n",
    "print(df_results)\n",
    "\n",
    "print(\"\\n=== Statistics ===\")\n",
    "print(f\"🎞️ Total videos processed     : {total_videos}\")\n",
    "print(f\"✅ Correct predictions        : {correct_preds}\")\n",
    "print(f\"❌ Incorrect predictions      : {incorrect_preds}\")\n",
    "if most_misclassified_class:\n",
    "    print(f\"⚠️ Most misclassified class   : {most_misclassified_class}\")\n",
    "\n",
    "# --- Optional: Save to CSV ---\n",
    "# df_results.to_csv(\"video_predictions_with_accuracy.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b84ff3f-5bac-42e6-94a1-4a9e4262f954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4aa8e9-4539-42a5-a260-b4bcc81eb2ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb8c9aa-7414-4aaa-ab75-fc035516372c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff11f14d-8d19-48b7-8e12-c4af041cc007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5463b9b5-dd74-4b82-8a80-189ddf96a369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164b5231-4fd3-49bc-93ca-bd3a77806c35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865fc0c8-fed3-4acf-b1a5-bb1f322f7184",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
